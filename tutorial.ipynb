{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679eff4-4711-4b75-9de8-2ffb11cdbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "pd.set_option(\"display.max_rows\",20)\n",
    "import matplotlib \n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991572d1-595c-46a5-834c-a98d1abdcfaa",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c8dad-712f-448e-9291-220bd73ad9d5",
   "metadata": {},
   "source": [
    "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n",
    "\n",
    "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
    "\n",
    "The Attributess include:\n",
    "- **Age**: age of the patient [years]\n",
    "- **Sex**: sex of the patient [M: Male, F: Female]\n",
    "- **ChestPainType**: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "- **RestingBP**: resting blood pressure [mm Hg]\n",
    "- **Cholesterol**: serum cholesterol [mm/dl]\n",
    "- **FastingBS**: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "- **RestingECG**: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "- **MaxHR**: maximum heart rate achieved [Numeric value between 60 and 202]\n",
    "- **ExerciseAngina**: exercise-induced angina [Y: Yes, N: No]\n",
    "- **Oldpeak**: oldpeak = ST [Numeric value measured in depression]\n",
    "- **ST_Slope**: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "- **HeartDisease**: output class [1: heart disease, 0: Normal]\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [Further dataset info on Kaggle.com](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
    "\n",
    "- [How do I plot it in Python?](https://python-graph-gallery.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04390d-5746-4e5a-9897-fe350bc108dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment the following line if you are running this notebook in Google Colab\n",
    "## Google Colab \n",
    "# !wget https://raw.githubusercontent.com/agreco92/tutorial_ML/main/data/heart_failure.csv\n",
    "# df = pd.read_csv(\"/content/heart_failure.csv\")\n",
    "\n",
    "## Github Codespaces\n",
    "# df = pd.read_csv(\"./data/heart_failure.csv\")\n",
    "\n",
    "## Local\n",
    "df = pd.read_csv(\"./data/heart_failure.csv\")\n",
    "df.head() # top 5 rows in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e0c5c-44d5-4579-a97f-e1b1128a802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # number of rows and columns in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d1f80-23a8-4e58-8fc6-cc5adfb1b3c6",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the first step in any machine learning workflow, as it allows to uncover patterns, spot anomalies, and check assumptions related to our data.\n",
    "\n",
    "The primary goal of EDA is to understand the data, gain insights, and inform model selection and feature engineering. Frequent steps in EDA are:\n",
    "\n",
    "- **Data Summary**: Understanding the basic information about the data like number of rows/columns, data types of the columns, and basic statistical details of numerical columns.\n",
    "\n",
    "- **Detect Missing Values**: Identifying missing data. This will require imputation or deletion, depending on the context (in this example, no missing values are expected)\n",
    "\n",
    "- **Univariate Analysis**: This involves analyzing each variable in the dataset individually. For numerical variables, this usually involves understanding the central tendency and spread, as well as detection of outliers. For categorical variables, it involves understanding the count or frequency of categories.\n",
    "\n",
    "- **Bivariate/Multivariate Analysis**: This involves analyzing the relationship between two or more variables, such as correlations among variables and their interactions\n",
    "\n",
    "- **Data Visualization**: Graphical representations of data are a key part of EDA. Plots such as bar plots, histograms, scatter plots, and box plots can help us understand the data better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8a066-fa37-4e60-9499-17cc3d32a236",
   "metadata": {},
   "source": [
    "## Column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898abb6-296f-43d2-848e-b7373935c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # list the type of data for each column of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94652514-20f8-46b5-84a9-93afe8163d80",
   "metadata": {},
   "source": [
    "Several categorical features are listed as \"object\" type. Here, we convert them to string to facilitate following steps.\n",
    "The remaining columns are labelled as numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bb9fe-6ba6-4f49-b4b3-ccff8aa90b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_col = df.select_dtypes(include=\"object\").columns # detect columns of type 'object'\n",
    "df[string_col] = df[string_col].astype(\"string\") # convert 'object' columns to string type\n",
    "string_col = string_col.to_list()\n",
    "\n",
    "num_col=df.columns.to_list() # all columns are included in the num_col variables\n",
    "\n",
    "# remove string columns using a loop\n",
    "for col in string_col: \n",
    "    num_col.remove(col) \n",
    "num_col.remove(\"HeartDisease\") # remove target variable from numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2da18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_col)\n",
    "print(string_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c297c8-2868-4554-ad43-85d676f4d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c23be-1f98-4e2c-ad55-a8a751fdd8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d19ec2-8ba7-4c89-8d09-073b39b2c0d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Numerical features analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b0f6c-cbd3-46e0-9a7d-b4b2721facd0",
   "metadata": {},
   "source": [
    "#### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9c16e-876b-4f6c-b5fe-1ca777ac5bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i,col in enumerate(num_col,1):\n",
    "    plt.subplot(4,3,i)\n",
    "    plt.title(f\"Distribution of {col} Data\")\n",
    "    sns.histplot(df[col],kde=True)\n",
    "    plt.tight_layout()\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aa2c9-019f-423a-bcc9-926824a5962c",
   "metadata": {},
   "source": [
    "By looking at the plots, we can conclude that:\n",
    "- numerical features span different ranges, thus require to be *scaled*\n",
    "- The `FastingBS` column is actually categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ce831-ffb2-4a28-8c86-43bb81e17697",
   "metadata": {},
   "source": [
    "#### Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c8f06-5555-4643-a259-671efce7cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr(method = 'spearman')\n",
    "\n",
    "# plot as heatmap\n",
    "plt.figure(figsize=(10, 9))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abdfec-30c5-4703-b7e9-933fbbf399ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.pairplot(df[num_col + ['HeartDisease']],hue=\"HeartDisease\",plot_kws={'s': 20} )\n",
    "plt.legend(\"HeartDisease\")\n",
    "plt.tight_layout()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973626d9-bdc5-41fa-8a79-1d7867a730a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bc874-ee11-4887-bd74-2f22678f3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the Distribution of Heat Diseases with respect to male and female\n",
    "fig=px.histogram(df, \n",
    "                 x = \"HeartDisease\",\n",
    "                 color = \"Sex\",\n",
    "                 hover_data = df.columns,\n",
    "                 title = \"Distribution of Heart Diseases\",\n",
    "                 barmode = \"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce36973-58b2-48e6-88e2-440cdfc9f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.histogram(df,\n",
    "                 x = \"ChestPainType\",\n",
    "                 color = \"HeartDisease\",\n",
    "                 hover_data = df.columns,\n",
    "                 title = \"Types of Chest Pain\"\n",
    "                )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7470bf7-4567-4003-b989-cb6dd5d15c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.histogram(df,\n",
    "                 x = \"ExerciseAngina\", color = 'HeartDisease',\n",
    "                 hover_data = df.columns,\n",
    "                 title = \"Distribution of ExerciseAngina\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa660a33-5a04-43d8-b5f2-ad1d56d17605",
   "metadata": {},
   "source": [
    "# Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9985c5-3a79-4138-8666-1fd59d4f7358",
   "metadata": {},
   "source": [
    "Numerical and categorical features require different preprocessing. Here, we distinguish between categorical and numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c53913-6312-4caa-8fc1-c6a8d9e48065",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'HeartDisease' \n",
    "feature_cols_categorical = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS']\n",
    "\n",
    "feature_cols_numeric = df.columns.to_list()\n",
    "feature_cols_numeric.remove(target_col) # remove target from numerical columns\n",
    "\n",
    "# remove string columns using a loop\n",
    "for categorical_column in feature_cols_categorical:\n",
    "    feature_cols_numeric.remove(categorical_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35db810-7038-4105-8db6-00dfed0ea2e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scale numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16cdb1-ed40-494b-8349-219ea4548923",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de043ab6-3389-4576-a07c-f04a61c3f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.RobustScaler(quantile_range = (1,99), unit_variance=True) # define scaling function\n",
    "scaled_df_numeric = scaler.fit_transform(df[num_col]) # scale numerical columns \n",
    "scaled_df_numeric = pd.DataFrame(scaled_df_numeric, columns= num_col) # create dataframe with scaled values\n",
    "df[num_col] = scaled_df_numeric.values # substitute original numerical features with scaled values\n",
    "df[num_col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530118b-af1c-4b87-be18-0d667697fed2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b8548-ffcb-4f9e-a3fc-461c013af37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a15da-63a1-4715-8ae5-a68f19ba6e3b",
   "metadata": {},
   "source": [
    "### Encode ordinal categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db364a-e29e-425a-8306-0c8f9901de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_features = ['ChestPainType', 'RestingECG'] # select categorical features for one-hot embedding\n",
    "\n",
    "# classify the rest of the categorical features as ordinal\n",
    "ordinal_features = feature_cols_categorical.copy()\n",
    "for ft in one_hot_features:\n",
    "    ordinal_features.remove(ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc298b-309c-45a4-b505-e42a76a552be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113eedd-21c4-4ce4-96b3-fb479c783137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal encoding \n",
    "df_ordinal_categories = df[ordinal_features].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df94da-4b5d-4d10-905b-391cc229cb5d",
   "metadata": {},
   "source": [
    "`ST_slope`, despite being categorical, has a natural ordering (Down,Flat,Up). In the following section, I correct the encoding to reflect this ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908512be-5dc9-4544-b16c-2987495d5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ordinal_categories['ST_Slope'].value_counts()) \n",
    "print(df['ST_Slope'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaa1b-35eb-4d04-a4a3-d60ec336d779",
   "metadata": {},
   "source": [
    "Encoding is not done properly! Let's fix it by explicitly setting the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262f92e-c715-4534-a3d7-12ff4b24ec16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define encoding function\n",
    "enc = OrdinalEncoder(categories=[['Down','Flat','Up']])  \n",
    "X = [['Up'], ['Flat'], ['Down']]\n",
    "enc.fit(X)\n",
    "\n",
    "# apply encoding to ST_Slope\n",
    "df_ordinal_categories['ST_Slope'] = np.ndarray.flatten(enc.transform([[s] for s in df['ST_Slope'].values]))\n",
    "# substitute values in the dataframe\n",
    "df[ordinal_features] = df_ordinal_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e13c33-08f0-4dc9-bb2d-b12480b64f44",
   "metadata": {},
   "source": [
    "### Encode other categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc3699-66f5-4faa-8f46-58dfe722720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac0d59-14fe-4e1c-98b8-a25b4575a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset before one-hot encoding\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205cbfc-852a-4199-bad8-c32ad220759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df,columns=one_hot_features,drop_first=True) # apply one-hot encoding on selected features\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ddfe0f-9dd7-40a0-b87d-6287a3b6346a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \"One-shot\" learning\n",
    "In this section, we will try to learn the target function using some of the algorithms learned so far. In order to have an idea of their generalization potential, we'll split the data and use the `_train` labels for training and the `_test`labels for estimating E_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91df12-629c-49af-b6de-5f315bb2eaea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08c0d6-7026-4751-b15f-343627d1fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df[df.columns.drop(target_col)], # input data\n",
    "                                                    df[target_col].values, # target column\n",
    "                                                    test_size=0.15,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425c0b8-a60f-4913-b3a6-d2f15f010446",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6051c04-387e-4df2-aeb9-cd730e4236e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(penalty = 'none', random_state =1, max_iter=1000) # define a perceptron classifier\n",
    "clf = clf.fit(X_train, Y_train) # fit the classifier using the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69b3ae-1f02-4715-a0b4-1e83afe29008",
   "metadata": {},
   "source": [
    "#### Testing error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af21a7-5b2c-4369-a35e-bae5105bd96a",
   "metadata": {},
   "source": [
    "***Exercise***: Compute *recall* and *precision* for the '1' label of the trained perceptron, thus compute the *f1 score* (see slides)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90ad62c9-7a39-477a-98c2-fd03564ee0d5",
   "metadata": {},
   "source": [
    "Y_pred = clf.predict(X_test) # predict the class using the trained classifier\n",
    "\n",
    "# hint: how to count all the positives, true_negatives\n",
    "all_positives = np.sum(Y_pred==1)\n",
    "true_negatives = np.sum((Y_pred==0) * (Y_test==0))\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1ae8c",
   "metadata": {},
   "source": [
    "Let's define a function that displays the confusion matrix for a classifier and quantifies its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e76469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf,X_test,Y_test, class_label = [\"No Disease\", \"Disease\"]):\n",
    "    # Make predictions\n",
    "    Y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "    # Create a DataFrame for the confusion matrix\n",
    "    df_cm = pd.DataFrame(cm, index=class_label, columns=class_label)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_cm, annot=True, linewidths=2, fmt='d')\n",
    "    plt.title(\"Confusion Matrix\", fontsize=25)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"Accuracy: \" + f\"{accuracy:.3f}\")\n",
    "    print(\"F1 score: \" + f\"{f1_score(Y_test, Y_pred):.3f}\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad3619-92f9-4068-9188-a4868f2f790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test) # predict the class using the trained classifier\n",
    "\n",
    "evaluate_classifier(clf,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a29aa-4eca-48ad-93cc-0d0849f8c235",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966bd6f",
   "metadata": {},
   "source": [
    "***Exercise***: Compute *recall* and *precision* for the '1' label of the trained perceptron, thus compute the *f1 score* (see slides)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1553def3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## SOLUTION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(....)\n",
    "# fit classifier\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766895b-77ff-41a2-8af7-902465f52f20",
   "metadata": {},
   "source": [
    "#### Testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca361f9d-eee7-4ce2-a7e7-512823943611",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test)\n",
    "\n",
    "evaluate_classifier(clf, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ac4b0-0710-4dff-b215-b3f3070ba971",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi-layer perceptron / Neural network (exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478f17c-edcf-47cc-aaa3-8abfd7abcdb2",
   "metadata": {},
   "source": [
    "***Exercise***: Using the known rule of thumb N_param <= 0.1* N_training points, design a neural network that outperforms the logistic classifier in the classification task. \n",
    "\n",
    "*Hint*: You can choose to disregard some features in order to increase the number of available parameters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a8cef03-5474-49d4-ae8c-748c96e7977e",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# YOUR CODE HERE: substitute any of \n",
    "# hidden_layer_sizes,learning_rate_init, activation\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = np.array([100,100,100,100,]),\n",
    "    alpha =0, \n",
    "    random_state =1, max_iter= 10000,solver='lbfgs')\n",
    "\n",
    "clf = clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b03b6-c170-4874-ac30-58ff29ece95e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184629a3-a1ef-4e90-a14d-3becf0357f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test)\n",
    "\n",
    "evaluate_classifier(clf, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb6db2-52ed-4b43-92ab-01d30c35f640",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the best model on full data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97a0c5-cee7-42d0-bbbf-305b4167d50d",
   "metadata": {},
   "source": [
    "***Exercise***: Train a neural network using the full dataset and report the training error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a59b0dd-032d-49b5-b4e8-e710fca8c4fe",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc5fa7",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c6c24-c48a-42ff-9856-2b2ab1b239f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2a144",
   "metadata": {},
   "source": [
    "## Estimate E_out using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c498787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[df.columns.drop(target_col)]\n",
    "y = df[target_col].values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 10)\n",
    "kf_split = kf.split(X = X, y = y)\n",
    "\n",
    "scores_log=[]\n",
    "\n",
    "for fold , (tran_index, validation_index) in enumerate(kf_split):\n",
    "    \n",
    "    # define training set \n",
    "    # hint: use df.loc[index, columns] to slice the dataframe\n",
    "    X_train = X.loc[tran_index, :]\n",
    "    y_train = y[tran_index]\n",
    "    \n",
    "    # define validation set\n",
    "    X_valid = X.loc[validation_index, :]\n",
    "    y_valid = y[validation_index]\n",
    "    \n",
    "    # instantiate logistic regressor\n",
    "    clf = LogisticRegression(random_state =1, \n",
    "                             max_iter=1000, penalty = 'none')\n",
    "    # train logistic regressor\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    # predict validation set\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    \n",
    "    # compute f1_score (function: f1_score)\n",
    "    score = accuracy_score(y_valid,y_pred)\n",
    "    \n",
    "    # add score to scores log\n",
    "    scores_log.append(score)\n",
    "\n",
    "print(np.mean(scores_log))\n",
    "print(np.std(scores_log))\n",
    "\n",
    "\n",
    "df_plot = pd.DataFrame({ 'accuracy' : scores_log})\n",
    "\n",
    "# boxplot\n",
    "ax = sns.boxplot(y='accuracy', data=df_plot)\n",
    "# add stripplot\n",
    "ax = sns.stripplot( y='accuracy', data=df_plot, color=\"orange\", jitter=0.2, size=10)\n",
    "\n",
    "# add title\n",
    "plt.title(\"Validation accuracy\", loc=\"left\")\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a705bd",
   "metadata": {},
   "source": [
    "## Hyperparameter estimation using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3f1e6",
   "metadata": {},
   "source": [
    "***Exercise***: Use cross validation to select both the degree of polynomial expansion and the regularization parameter to maximize the performance of a logistic regression model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "596b1af2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]  # List of degrees to try\n",
    "C_values = [1e-05, 1e-03, 1e-02, 1, 100, 1e05]  # List of C values to try\n",
    "\n",
    "score_matrix = np.zeros((len(degrees), len(C_values)))\n",
    "\n",
    "X = df[df.columns.drop(target_col)]\n",
    "y = df[target_col]\n",
    "\n",
    "for (..loop over degrees..):\n",
    "    poly_features = PolynomialFeatures(degree=degrees[i])\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    for (... loop over C values)...:\n",
    "# your code here\n",
    "       scores = cross_val_score(\n",
    "            X = X_poly, \n",
    "            y = y,\n",
    "            estimator = LogisticRegression(C=..., max_iter=10000, \n",
    "                               penalty='l2', solver = \"lbfgs\"),\n",
    "            cv=10, n_jobs = -1, scoring='accuracy')\n",
    "        avg_score = .....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the matrix using seaborn heatmap\n",
    "sns.heatmap(score_matrix, xticklabels=C_values, yticklabels=degrees, \n",
    "            annot=True)\n",
    "plt.xlabel('C Values')\n",
    "plt.ylabel('Degrees')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
